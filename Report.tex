\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{listings}

\title{Report Cloud And BigData}
\author{RÃ©mi DE BAUDRY D'ASSON & Arthur BRUGIERE & Tom HERBRETEAU }
\date{December 2018}

\begin{document}

\maketitle

\section{Introduction}

As presented in the subject, the goal of this project is to use a Spark infrastructure inside Docker containers to simulate a cluster of virtual machines. \\

In our case, we will execute a wordcount application on a big text file, according to several scenarios, then we will measure and compare the performances of different architectures to figure where the improvement or non improvement comes from.

\section{Scenarios}

In this section, we will describe the different scenarios we will implement to test our application, from the most simple to the most complicated. \\

For each scenario, we will test it's performance on a big text file (2Go), then compare the execution time.



\subsection{Scenario 1 : One Spark container running locally}

The first version of the scenario consist in using just one Docker container with Spark, which is the equivalent of running Spark locally on our computer. \\

To launch this scenario, simply go into \textbf{V2} folder and execute the script \textbf{launch.sh}. The script will do the following:
\begin{enumerate}
    \item Run Docker-compose:
    \begin{quote}
\begin{verbatim}
sudo docker-compose up -d
\end{verbatim}
    \end{quote}
    
    \item Launch work in master container:
    \begin{quote}
\begin{verbatim}
sudo docker exec -ti v1_spark_1 sh -c 
"/usr/spark-2.3.1/bin/spark-submit --class 
WordCount --master local /tmp/data/wc.jar 
/tmp/data/sample1.txt"
\end{verbatim}
    \end{quote}
    
    \item And finally display the result:
    \begin{quote}
\begin{verbatim}
cat ./data/result/*
\end{verbatim}
    \end{quote}
\end{enumerate}
\ \

\noindent In this configuration, with our 2Go file we get an execution time of \textbf{0ms}.



\subsection{Scenario 2 : Several Spark containers running with one master and several workers}

The second version of the scenario uses this time several Spark containers:
\begin{itemize}
  \item One will be the master
  \item The others will be the workers
\end{itemize}
\ \

\noindent To launch this scenario, simply go into \textbf{V2} folder and execute the script \textbf{launch.sh}. You will be asked how many workers you want to start and how many core you want to affect to each worker, then the script does the following:
\begin{enumerate}
    \item Run Docker-compose and specifying the number of workers:
    \begin{quote}
\begin{verbatim}
sudo docker-compose up -d --scale worker=$nbrWorkers
\end{verbatim}
    \end{quote}
    
    \item Launch work in master container:
    \begin{quote}
\begin{verbatim}
sudo docker exec -ti v2_master_1 sh -c 
"/usr/spark-2.3.1/bin/spark-submit --class WordCount --master 
spark://master:7077 /tmp/data/wc.jar /tmp/data/sample1.txt"
\end{verbatim}
    \end{quote}
    
    \item And finally display the result:
    \begin{quote}
\begin{verbatim}
cat ./data/result/*
\end{verbatim}
    \end{quote}
\end{enumerate}
\ \

\noindent We get the following results with different number of workers: \\
\begin{center}
    \begin{tabular}{||c c c c||} 
    \hline
    FILE SIZE & NUM WORKER & NUM CORE/WORKER & TIME \\ [0.5ex] 
    \hline\hline
    2Go & 1 & 1 & \textbf{0ms} \\ 
    \hline
    2Go & 1 & 2 & \textbf{0ms} \\ 
    \hline
    2Go & 1 & 3 & \textbf{0ms} \\ 
    \hline
    2Go & 1 & 4 & \textbf{0ms} \\ 
    \hline
    2Go & 2 & 1 & \textbf{0ms} \\ 
    \hline
    2Go & 2 & 2 & \textbf{0ms} \\ 
    \hline
    2Go & 3 & 1 & \textbf{0ms} \\ 
    \hline
    \end{tabular}
\end{center}
\vspace*{0.2cm}


\subsection{Scenario 3 : Same as previous + several HDFS containers}

\section{Conclusion}

\end{document}
